{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import walk, makedirs\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert-base-multilingual-cased-sentiment-multilingual':                                                 model       traindata  \\\n",
       " 20  bert-base-multilingual-cased-sentiment-multili...   1epoch_train8   \n",
       " 11  bert-base-multilingual-cased-sentiment-multili...   3epoch_train4   \n",
       " 23  bert-base-multilingual-cased-sentiment-multili...   3epoch_train2   \n",
       " 16  bert-base-multilingual-cased-sentiment-multili...   2epoch_train4   \n",
       " 27  bert-base-multilingual-cased-sentiment-multili...   2epoch_train8   \n",
       " 8   bert-base-multilingual-cased-sentiment-multili...   1epoch_train2   \n",
       " 19  bert-base-multilingual-cased-sentiment-multili...  4epoch_train22   \n",
       " 28  bert-base-multilingual-cased-sentiment-multili...   4epoch_train8   \n",
       " 18  bert-base-multilingual-cased-sentiment-multili...   5epoch_train8   \n",
       " 5   bert-base-multilingual-cased-sentiment-multili...  4epoch_train16   \n",
       " 7   bert-base-multilingual-cased-sentiment-multili...  5epoch_train16   \n",
       " 2   bert-base-multilingual-cased-sentiment-multili...  1epoch_train22   \n",
       " 14  bert-base-multilingual-cased-sentiment-multili...   5epoch_train2   \n",
       " 1   bert-base-multilingual-cased-sentiment-multili...  4epoch_train28   \n",
       " 6   bert-base-multilingual-cased-sentiment-multili...   2epoch_train2   \n",
       " 13  bert-base-multilingual-cased-sentiment-multili...   3epoch_train8   \n",
       " 3   bert-base-multilingual-cased-sentiment-multili...   1epoch_train4   \n",
       " 22  bert-base-multilingual-cased-sentiment-multili...   4epoch_train4   \n",
       " 9   bert-base-multilingual-cased-sentiment-multili...  3epoch_train28   \n",
       " 10  bert-base-multilingual-cased-sentiment-multili...  2epoch_train28   \n",
       " 21  bert-base-multilingual-cased-sentiment-multili...  1epoch_train16   \n",
       " 17  bert-base-multilingual-cased-sentiment-multili...  5epoch_train28   \n",
       " 12  bert-base-multilingual-cased-sentiment-multili...  2epoch_train22   \n",
       " 25  bert-base-multilingual-cased-sentiment-multili...   4epoch_train2   \n",
       " 15  bert-base-multilingual-cased-sentiment-multili...   5epoch_train4   \n",
       " 24  bert-base-multilingual-cased-sentiment-multili...  3epoch_train22   \n",
       " 29  bert-base-multilingual-cased-sentiment-multili...  2epoch_train16   \n",
       " 4   bert-base-multilingual-cased-sentiment-multili...  3epoch_train16   \n",
       " 26  bert-base-multilingual-cased-sentiment-multili...  1epoch_train28   \n",
       " 0   bert-base-multilingual-cased-sentiment-multili...  5epoch_train22   \n",
       " \n",
       "     accuracy  std_accuracy  precision  std_precision    recall  std_recall  \\\n",
       " 20  0.857143      0.074568   0.864596       0.066309  0.857143    0.074568   \n",
       " 11  0.869048      0.068630   0.882879       0.046034  0.869048    0.068630   \n",
       " 23  0.828869      0.060939   0.861468       0.041579  0.828869    0.060939   \n",
       " 16  0.860119      0.065052   0.875455       0.042915  0.860119    0.065052   \n",
       " 27  0.876488      0.087063   0.883990       0.080157  0.876488    0.087063   \n",
       " 8   0.812500      0.058378   0.842595       0.028578  0.812500    0.058378   \n",
       " 19  0.940476      0.034097   0.940849       0.034557  0.940476    0.034097   \n",
       " 28  0.880952      0.089323   0.890547       0.079780  0.880952    0.089323   \n",
       " 18  0.883929      0.089620   0.891134       0.082776  0.883929    0.089620   \n",
       " 5   0.943452      0.029046   0.944017       0.029601  0.943452    0.029046   \n",
       " 7   0.944940      0.025385   0.945677       0.025802  0.944940    0.025385   \n",
       " 2   0.931548      0.021097   0.933101       0.019931  0.931548    0.021097   \n",
       " 14  0.836310      0.055460   0.858477       0.046064  0.836310    0.055460   \n",
       " 1   0.950893      0.027155   0.951234       0.027004  0.950893    0.027155   \n",
       " 6   0.809524      0.043817   0.855289       0.028016  0.809524    0.043817   \n",
       " 13  0.873512      0.076979   0.882753       0.070433  0.873512    0.076979   \n",
       " 3   0.849702      0.069352   0.867198       0.040873  0.849702    0.069352   \n",
       " 22  0.869048      0.068630   0.884370       0.047335  0.869048    0.068630   \n",
       " 9   0.944940      0.032295   0.944952       0.032285  0.944940    0.032295   \n",
       " 10  0.921131      0.018586   0.921990       0.019236  0.921131    0.018586   \n",
       " 21  0.918155      0.059282   0.920855       0.057014  0.918155    0.059282   \n",
       " 17  0.953869      0.024587   0.954212       0.024413  0.953869    0.024587   \n",
       " 12  0.937500      0.035434   0.937759       0.035522  0.937500    0.035434   \n",
       " 25  0.830357      0.052633   0.855685       0.043512  0.830357    0.052633   \n",
       " 15  0.864583      0.068339   0.881912       0.047210  0.864583    0.068339   \n",
       " 24  0.940476      0.031356   0.941394       0.031947  0.940476    0.031356   \n",
       " 29  0.941964      0.031250   0.943703       0.029790  0.941964    0.031250   \n",
       " 4   0.941964      0.043052   0.942203       0.042945  0.941964    0.043052   \n",
       " 26  0.922619      0.018042   0.924128       0.016260  0.922619    0.018042   \n",
       " 0   0.940476      0.034097   0.940646       0.034223  0.940476    0.034097   \n",
       " \n",
       "           f1    std_f1  \n",
       " 20  0.855926  0.076164  \n",
       " 11  0.866833  0.072378  \n",
       " 23  0.823933  0.065652  \n",
       " 16  0.857684  0.069007  \n",
       " 27  0.875434  0.088368  \n",
       " 8   0.806894  0.065650  \n",
       " 19  0.940470  0.034094  \n",
       " 28  0.879598  0.091121  \n",
       " 18  0.882933  0.090976  \n",
       " 5   0.943441  0.029044  \n",
       " 7   0.944921  0.025390  \n",
       " 2   0.931474  0.021169  \n",
       " 14  0.833222  0.058345  \n",
       " 1   0.950882  0.027163  \n",
       " 6   0.802544  0.048722  \n",
       " 13  0.872325  0.078542  \n",
       " 3   0.846509  0.074706  \n",
       " 22  0.866750  0.072307  \n",
       " 9   0.944940  0.032296  \n",
       " 10  0.921096  0.018574  \n",
       " 21  0.917954  0.059456  \n",
       " 17  0.953858  0.024596  \n",
       " 12  0.937492  0.035437  \n",
       " 25  0.826802  0.055371  \n",
       " 15  0.862031  0.072239  \n",
       " 24  0.940452  0.031352  \n",
       " 29  0.941886  0.031330  \n",
       " 4   0.941954  0.043062  \n",
       " 26  0.922535  0.018147  \n",
       " 0   0.940472  0.034098  ,\n",
       " 'dehatebert-mono-french':                      model       traindata  accuracy  std_accuracy  precision  \\\n",
       " 5   dehatebert-mono-french  4epoch_train16  0.873512      0.011235   0.886571   \n",
       " 10  dehatebert-mono-french  2epoch_train28  0.913690      0.026158   0.913803   \n",
       " 15  dehatebert-mono-french   5epoch_train4  0.811012      0.059950   0.822807   \n",
       " 11  dehatebert-mono-french   3epoch_train4  0.788690      0.074746   0.809503   \n",
       " 29  dehatebert-mono-french  2epoch_train16  0.882440      0.011235   0.882658   \n",
       " 8   dehatebert-mono-french   1epoch_train2  0.714286      0.079359   0.772570   \n",
       " 27  dehatebert-mono-french   2epoch_train8  0.870536      0.027155   0.871141   \n",
       " 24  dehatebert-mono-french  3epoch_train22  0.883929      0.017857   0.887662   \n",
       " 3   dehatebert-mono-french   1epoch_train4  0.806548      0.041239   0.822404   \n",
       " 12  dehatebert-mono-french  2epoch_train22  0.901786      0.017857   0.902613   \n",
       " 4   dehatebert-mono-french  3epoch_train16  0.875000      0.007732   0.877914   \n",
       " 9   dehatebert-mono-french  3epoch_train28  0.904762      0.026158   0.905209   \n",
       " 0   dehatebert-mono-french  5epoch_train22  0.886905      0.022470   0.893335   \n",
       " 16  dehatebert-mono-french   2epoch_train4  0.819940      0.097943   0.826913   \n",
       " 18  dehatebert-mono-french   5epoch_train8  0.852679      0.033705   0.859751   \n",
       " 28  dehatebert-mono-french   4epoch_train8  0.852679      0.032192   0.860592   \n",
       " 22  dehatebert-mono-french   4epoch_train4  0.812500      0.061374   0.823243   \n",
       " 23  dehatebert-mono-french   3epoch_train2  0.741071      0.108345   0.801760   \n",
       " 7   dehatebert-mono-french  5epoch_train16  0.886905      0.021097   0.896914   \n",
       " 1   dehatebert-mono-french  4epoch_train28  0.910714      0.016096   0.912343   \n",
       " 26  dehatebert-mono-french  1epoch_train28  0.918155      0.002577   0.919012   \n",
       " 17  dehatebert-mono-french  5epoch_train28  0.898810      0.014351   0.902696   \n",
       " 25  dehatebert-mono-french   4epoch_train2  0.739583      0.106988   0.799098   \n",
       " 2   dehatebert-mono-french  1epoch_train22  0.907738      0.013639   0.908357   \n",
       " 20  dehatebert-mono-french   1epoch_train8  0.860119      0.046466   0.865814   \n",
       " 6   dehatebert-mono-french   2epoch_train2  0.712798      0.106988   0.775346   \n",
       " 21  dehatebert-mono-french  1epoch_train16  0.889881      0.020620   0.891039   \n",
       " 19  dehatebert-mono-french  4epoch_train22  0.882440      0.026909   0.889668   \n",
       " 13  dehatebert-mono-french   3epoch_train8  0.870536      0.019459   0.874613   \n",
       " 14  dehatebert-mono-french   5epoch_train2  0.745536      0.100521   0.800735   \n",
       " \n",
       "     std_precision    recall  std_recall        f1    std_f1  \n",
       " 5        0.008096  0.873512    0.011235  0.872391  0.012136  \n",
       " 10       0.026232  0.913690    0.026158  0.913685  0.026155  \n",
       " 15       0.057151  0.811012    0.059950  0.809053  0.061113  \n",
       " 11       0.064696  0.788690    0.074746  0.784169  0.078759  \n",
       " 29       0.011460  0.882440    0.011235  0.882425  0.011220  \n",
       " 8        0.022929  0.714286    0.079359  0.692791  0.113231  \n",
       " 27       0.027273  0.870536    0.027155  0.870484  0.027158  \n",
       " 24       0.018645  0.883929    0.017857  0.883654  0.017845  \n",
       " 3        0.056267  0.806548    0.041239  0.804623  0.039759  \n",
       " 12       0.016875  0.901786    0.017857  0.901726  0.017931  \n",
       " 4        0.009840  0.875000    0.007732  0.874767  0.007618  \n",
       " 9        0.026066  0.904762    0.026158  0.904734  0.026172  \n",
       " 0        0.021077  0.886905    0.022470  0.886419  0.022670  \n",
       " 16       0.099277  0.819940    0.097943  0.818948  0.098378  \n",
       " 18       0.029980  0.852679    0.033705  0.851845  0.034478  \n",
       " 28       0.028689  0.852679    0.032192  0.851770  0.032929  \n",
       " 22       0.060951  0.812500    0.061374  0.810847  0.061934  \n",
       " 23       0.018258  0.741071    0.108345  0.718153  0.145941  \n",
       " 7        0.021649  0.886905    0.021097  0.886177  0.021300  \n",
       " 1        0.017193  0.910714    0.016096  0.910634  0.016053  \n",
       " 26       0.002624  0.918155    0.002577  0.918113  0.002578  \n",
       " 17       0.014426  0.898810    0.014351  0.898562  0.014409  \n",
       " 25       0.014339  0.739583    0.106988  0.716821  0.144751  \n",
       " 2        0.014156  0.907738    0.013639  0.907707  0.013615  \n",
       " 20       0.048441  0.860119    0.046466  0.859609  0.046448  \n",
       " 6        0.003326  0.712798    0.106988  0.684855  0.154954  \n",
       " 21       0.021577  0.889881    0.020620  0.889809  0.020567  \n",
       " 19       0.026351  0.882440    0.026909  0.881877  0.027087  \n",
       " 13       0.019844  0.870536    0.019459  0.870184  0.019501  \n",
       " 14       0.011979  0.745536    0.100521  0.725460  0.134204  ,\n",
       " 'french_xlm_xnli':               model       traindata  accuracy  std_accuracy  precision  \\\n",
       " 1   french_xlm_xnli  4epoch_train28  0.964286      0.004464   0.964659   \n",
       " 14  french_xlm_xnli   5epoch_train2  0.916667      0.040013   0.918954   \n",
       " 21  french_xlm_xnli  1epoch_train16  0.949405      0.002577   0.950584   \n",
       " 23  french_xlm_xnli   3epoch_train2  0.903274      0.044717   0.907965   \n",
       " 25  french_xlm_xnli   4epoch_train2  0.910714      0.043052   0.913224   \n",
       " 9   french_xlm_xnli  3epoch_train28  0.967262      0.005155   0.967914   \n",
       " 7   french_xlm_xnli  5epoch_train16  0.967262      0.002577   0.968180   \n",
       " 12  french_xlm_xnli  2epoch_train22  0.965774      0.006819   0.966475   \n",
       " 15  french_xlm_xnli   5epoch_train4  0.934524      0.034097   0.939050   \n",
       " 10  french_xlm_xnli  2epoch_train28  0.967262      0.002577   0.967936   \n",
       " 11  french_xlm_xnli   3epoch_train4  0.934524      0.032295   0.940691   \n",
       " 20  french_xlm_xnli   1epoch_train8  0.909226      0.045600   0.911821   \n",
       " 28  french_xlm_xnli   4epoch_train8  0.943452      0.009293   0.945415   \n",
       " 29  french_xlm_xnli  2epoch_train16  0.956845      0.002577   0.957343   \n",
       " 27  french_xlm_xnli   2epoch_train8  0.927083      0.022909   0.927372   \n",
       " 0   french_xlm_xnli  5epoch_train22  0.970238      0.002577   0.971364   \n",
       " 17  french_xlm_xnli  5epoch_train28  0.964286      0.004464   0.964659   \n",
       " 24  french_xlm_xnli  3epoch_train22  0.968750      0.004464   0.969509   \n",
       " 5   french_xlm_xnli  4epoch_train16  0.965774      0.002577   0.966580   \n",
       " 6   french_xlm_xnli   2epoch_train2  0.897321      0.040916   0.903586   \n",
       " 16  french_xlm_xnli   2epoch_train4  0.936012      0.034676   0.941061   \n",
       " 26  french_xlm_xnli  1epoch_train28  0.961310      0.006819   0.961480   \n",
       " 13  french_xlm_xnli   3epoch_train8  0.946429      0.011811   0.947427   \n",
       " 3   french_xlm_xnli   1epoch_train4  0.918155      0.043360   0.920127   \n",
       " 22  french_xlm_xnli   4epoch_train4  0.937500      0.038143   0.942859   \n",
       " 4   french_xlm_xnli  3epoch_train16  0.961310      0.006819   0.961771   \n",
       " 19  french_xlm_xnli  4epoch_train22  0.971726      0.006819   0.972487   \n",
       " 18  french_xlm_xnli   5epoch_train8  0.944940      0.010310   0.946867   \n",
       " 8   french_xlm_xnli   1epoch_train2  0.885417      0.027277   0.893242   \n",
       " 2   french_xlm_xnli  1epoch_train22  0.961310      0.005155   0.962137   \n",
       " \n",
       "     std_precision    recall  std_recall        f1    std_f1  \n",
       " 1        0.004918  0.964286    0.004464  0.964279  0.004457  \n",
       " 14       0.041996  0.916667    0.040013  0.916587  0.039959  \n",
       " 21       0.003425  0.949405    0.002577  0.949372  0.002564  \n",
       " 23       0.047482  0.903274    0.044717  0.903049  0.044633  \n",
       " 25       0.045397  0.910714    0.043052  0.910624  0.042984  \n",
       " 9        0.005778  0.967262    0.005155  0.967252  0.005147  \n",
       " 7        0.001319  0.967262    0.002577  0.967245  0.002602  \n",
       " 12       0.006131  0.965774    0.006819  0.965759  0.006836  \n",
       " 15       0.029889  0.934524    0.034097  0.934286  0.034400  \n",
       " 10       0.003164  0.967262    0.002577  0.967251  0.002570  \n",
       " 11       0.027389  0.934524    0.032295  0.934217  0.032676  \n",
       " 20       0.042941  0.909226    0.045600  0.909013  0.045899  \n",
       " 28       0.009042  0.943452    0.009293  0.943388  0.009321  \n",
       " 29       0.002255  0.956845    0.002577  0.956833  0.002587  \n",
       " 27       0.023026  0.927083    0.022909  0.927072  0.022905  \n",
       " 0        0.002173  0.970238    0.002577  0.970220  0.002586  \n",
       " 17       0.004918  0.964286    0.004464  0.964279  0.004457  \n",
       " 24       0.004115  0.968750    0.004464  0.968737  0.004473  \n",
       " 5        0.002252  0.965774    0.002577  0.965758  0.002590  \n",
       " 6        0.042654  0.897321    0.040916  0.896943  0.040929  \n",
       " 16       0.030495  0.936012    0.034676  0.935758  0.034980  \n",
       " 26       0.006741  0.961310    0.006819  0.961306  0.006821  \n",
       " 13       0.010283  0.946429    0.011811  0.946391  0.011871  \n",
       " 3        0.042864  0.918155    0.043360  0.918044  0.043450  \n",
       " 22       0.032196  0.937500    0.038143  0.937205  0.038527  \n",
       " 4        0.006282  0.961310    0.006819  0.961298  0.006833  \n",
       " 19       0.006306  0.971726    0.006819  0.971713  0.006831  \n",
       " 18       0.009650  0.944940    0.010310  0.944878  0.010349  \n",
       " 8        0.028058  0.885417    0.027277  0.884823  0.027490  \n",
       " 2        0.003850  0.961310    0.005155  0.961290  0.005187  ,\n",
       " 'xlm-roberta-base-sentiment-multilingual':                                       model       traindata  accuracy  \\\n",
       " 6   xlm-roberta-base-sentiment-multilingual   2epoch_train2  0.776786   \n",
       " 28  xlm-roberta-base-sentiment-multilingual   4epoch_train8  0.872024   \n",
       " 16  xlm-roberta-base-sentiment-multilingual   2epoch_train4  0.852679   \n",
       " 10  xlm-roberta-base-sentiment-multilingual  2epoch_train28  0.898810   \n",
       " 5   xlm-roberta-base-sentiment-multilingual  4epoch_train16  0.891369   \n",
       " 17  xlm-roberta-base-sentiment-multilingual  5epoch_train28  0.922619   \n",
       " 14  xlm-roberta-base-sentiment-multilingual   5epoch_train2  0.827381   \n",
       " 11  xlm-roberta-base-sentiment-multilingual   3epoch_train4  0.879464   \n",
       " 0   xlm-roberta-base-sentiment-multilingual  5epoch_train22  0.919643   \n",
       " 8   xlm-roberta-base-sentiment-multilingual   1epoch_train2  0.742560   \n",
       " 13  xlm-roberta-base-sentiment-multilingual   3epoch_train8  0.877976   \n",
       " 3   xlm-roberta-base-sentiment-multilingual   1epoch_train4  0.846726   \n",
       " 20  xlm-roberta-base-sentiment-multilingual   1epoch_train8  0.840774   \n",
       " 9   xlm-roberta-base-sentiment-multilingual  3epoch_train28  0.903274   \n",
       " 24  xlm-roberta-base-sentiment-multilingual  3epoch_train22  0.903274   \n",
       " 25  xlm-roberta-base-sentiment-multilingual   4epoch_train2  0.806548   \n",
       " 1   xlm-roberta-base-sentiment-multilingual  4epoch_train28  0.916667   \n",
       " 27  xlm-roberta-base-sentiment-multilingual   2epoch_train8  0.864583   \n",
       " 19  xlm-roberta-base-sentiment-multilingual  4epoch_train22  0.919643   \n",
       " 2   xlm-roberta-base-sentiment-multilingual  1epoch_train22  0.864583   \n",
       " 4   xlm-roberta-base-sentiment-multilingual  3epoch_train16  0.889881   \n",
       " 26  xlm-roberta-base-sentiment-multilingual  1epoch_train28  0.898810   \n",
       " 22  xlm-roberta-base-sentiment-multilingual   4epoch_train4  0.882440   \n",
       " 23  xlm-roberta-base-sentiment-multilingual   3epoch_train2  0.794643   \n",
       " 29  xlm-roberta-base-sentiment-multilingual  2epoch_train16  0.888393   \n",
       " 15  xlm-roberta-base-sentiment-multilingual   5epoch_train4  0.873512   \n",
       " 18  xlm-roberta-base-sentiment-multilingual   5epoch_train8  0.877976   \n",
       " 21  xlm-roberta-base-sentiment-multilingual  1epoch_train16  0.860119   \n",
       " 12  xlm-roberta-base-sentiment-multilingual  2epoch_train22  0.900298   \n",
       " 7   xlm-roberta-base-sentiment-multilingual  5epoch_train16  0.903274   \n",
       " \n",
       "     std_accuracy  precision  std_precision    recall  std_recall        f1  \\\n",
       " 6       0.096886   0.823380       0.040963  0.776786    0.096886  0.763121   \n",
       " 28      0.051355   0.881806       0.038176  0.872024    0.051355  0.870762   \n",
       " 16      0.015465   0.860105       0.013975  0.852679    0.015465  0.851895   \n",
       " 10      0.009293   0.904940       0.011111  0.898810    0.009293  0.898431   \n",
       " 5       0.002577   0.893682       0.002048  0.891369    0.002577  0.891208   \n",
       " 17      0.037173   0.923934       0.037391  0.922619    0.037173  0.922561   \n",
       " 14      0.051549   0.848304       0.031710  0.827381    0.051549  0.823942   \n",
       " 11      0.031250   0.883654       0.027702  0.879464    0.031250  0.879067   \n",
       " 0       0.011811   0.922801       0.011086  0.919643    0.011811  0.919487   \n",
       " 8       0.090652   0.795369       0.048830  0.742560    0.090652  0.725205   \n",
       " 13      0.044717   0.886649       0.033699  0.877976    0.044717  0.876984   \n",
       " 3       0.011235   0.852856       0.009174  0.846726    0.011235  0.846039   \n",
       " 20      0.034097   0.848918       0.024585  0.840774    0.034097  0.839600   \n",
       " 9       0.014351   0.909512       0.014542  0.903274    0.014351  0.902903   \n",
       " 24      0.018586   0.905497       0.016830  0.903274    0.018586  0.903124   \n",
       " 25      0.068339   0.833385       0.035217  0.806548    0.068339  0.800632   \n",
       " 1       0.018586   0.918926       0.019758  0.916667    0.018586  0.916564   \n",
       " 27      0.047735   0.876596       0.028921  0.864583    0.047735  0.862926   \n",
       " 19      0.004464   0.922668       0.001673  0.919643    0.004464  0.919493   \n",
       " 2       0.013639   0.873584       0.006106  0.864583    0.013639  0.863706   \n",
       " 4       0.009293   0.891632       0.007931  0.889881    0.009293  0.889750   \n",
       " 26      0.041480   0.903376       0.037561  0.898810    0.041480  0.898434   \n",
       " 22      0.029725   0.886954       0.023679  0.882440    0.029725  0.881990   \n",
       " 23      0.089174   0.840228       0.035824  0.794643    0.089174  0.783467   \n",
       " 29      0.007732   0.889937       0.007757  0.888393    0.007732  0.888282   \n",
       " 15      0.037968   0.876235       0.034959  0.873512    0.037968  0.873208   \n",
       " 18      0.053819   0.887546       0.039479  0.877976    0.053819  0.876734   \n",
       " 21      0.032907   0.866391       0.026847  0.860119    0.032907  0.859386   \n",
       " 12      0.005155   0.905085       0.006474  0.900298    0.005155  0.900004   \n",
       " 7       0.006819   0.904910       0.005509  0.903274    0.006819  0.903171   \n",
       " \n",
       "       std_f1  \n",
       " 6   0.117470  \n",
       " 28  0.053199  \n",
       " 16  0.015770  \n",
       " 10  0.009254  \n",
       " 5   0.002674  \n",
       " 17  0.037190  \n",
       " 14  0.055897  \n",
       " 11  0.031636  \n",
       " 0   0.011872  \n",
       " 8   0.112740  \n",
       " 13  0.046087  \n",
       " 3   0.011593  \n",
       " 20  0.035689  \n",
       " 9   0.014423  \n",
       " 24  0.018713  \n",
       " 25  0.077187  \n",
       " 1   0.018563  \n",
       " 27  0.050455  \n",
       " 19  0.004607  \n",
       " 2   0.014400  \n",
       " 4   0.009417  \n",
       " 26  0.041806  \n",
       " 22  0.030392  \n",
       " 23  0.105704  \n",
       " 29  0.007741  \n",
       " 15  0.038341  \n",
       " 18  0.055778  \n",
       " 21  0.033665  \n",
       " 12  0.005138  \n",
       " 7   0.006916  }"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir = '../results/fewshot'\n",
    "fewshot_names =['bert-base-multilingual-cased-sentiment-multilingual',\n",
    "                'dehatebert-mono-french',\n",
    "                'french_xlm_xnli',\n",
    "                'xlm-roberta-base-sentiment-multilingual']\n",
    "result_dict = {}\n",
    "\n",
    "for name in fewshot_names:\n",
    "  result_dict.update({name:pd.read_csv(f\"{results_dir}/{name}\").sample(frac=1)})\n",
    "\n",
    "result_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### make line graph for train size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(layout='constrained')\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "for key, value in output_dict.items():\n",
    "  rects = ax.bar(x + offset, b, width, label=key)\n",
    "  # rects = ax.bar(x + offset, list(value.get('a_mean')), width, label=key)\n",
    "  # ax.bar_label(rects, padding=3)\n",
    "  multiplier += 1\n",
    "\n",
    "ax.set_ylabel('Time in ms')\n",
    "ax.set_title('Problem2_B_values')\n",
    "\n",
    "no_thread = deepcopy(colnames)\n",
    "no_thread.remove(\"thread\")\n",
    "\n",
    "ax.set_xticks(x + width, no_thread)\n",
    "ax.legend(keys)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
