{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-03-20 14:16:00.013800: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-20 14:16:02.098552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/lib64\n","2023-03-20 14:16:02.098714: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/lib64\n","2023-03-20 14:16:02.098725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["from os import walk, makedirs\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import SGDClassifier\n","from sentence_transformers import SentenceTransformer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### function to read all data in from data_ready/  \n","data is already split in to k folds"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def read_data_for_fold(k: int) -> dict:\n","  data_dict = {}\n","  k_fold_dir = f\"../data_ready/k_is_{str(k)}/\"\n","  # print (os.path(data_ready_dir))\n","  filenames = []\n","  for (_, _, name) in walk(k_fold_dir): \n","    filenames.extend(name)\n","\n","  for fname in filenames:\n","    dataframe = pd.read_csv(k_fold_dir+fname).sample(frac=1)\n","    dict_key = fname.split(\".csv\")[0]\n","    data_dict.update({dict_key : dataframe})\n","\n","  return data_dict"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### output file genertic fuction ####"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def predictions_to_csv(test_text: list, test_class: list, predictions: list, k: int, train_filename: str, pred_output_dir: str):\n","  # output predictions to csv files.\n","  output_df = pd.DataFrame()\n","  output_df[\"text\"] = test_text\n","  output_df[\"class\"] = test_class\n","  # add new predictions as a 3rd column\n","  # dataframe has these columns (test, class, prediction).\n","  output_df[\"predicted\"] = predictions \n","\n","  makedirs(f\"../predictions/k_is_{k}/{pred_output_dir}/\", exist_ok=True)\n","  output_df.to_csv(f\"../predictions/k_is_{k}/{pred_output_dir}/{train_filename}\", index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Baseline test function  \n","this is called once for every train/test pair\n","\n","a note on the train files,  \n","train2.csv contains 8 train data because it contains...\n","- (one english inclusive\n","- one english exclusive\n","- one french inclusive\n","- one french exclusive) * 2 = 8  \n","  \n","this information does not affect anything in this function, it is just good to know.\n","\n","using these extremely small datasets shows the power of fewshot with setfit.  \n","For the fewshot i have done, we get 90%+ with this small data.  \n","hopefully these baselines predict very bad."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def do_baseline_tests(test_text: list, test_class: list, train_text: list, train_class: list, k: int, train_filename: str, Tfidf: TfidfVectorizer, sentnece_transformer: SentenceTransformer):\n","  ########################## begin basline tests ##########################\n","  \"\"\"\n","    Baseline models here.\n","    test_text, test_class, train_text, train_class are lists\n","    classes are balanced.\n","  \"\"\"\n","  ############################\n","  ####  TF-IDF encodings  ####\n","  train_tfidf = Tfidf.fit_transform(train_text).toarray()\n","  test_tfidf = Tfidf.transform(test_text).toarray()\n","  ############################\n","\n","  ############################\n","  ####   BERT encodings   ####\n","  train_encodings = sentnece_transformer.encode(train_text)\n","  test_encodings = sentnece_transformer.encode(test_text)\n","  ############################\n","\n","  ### Gaussian NaiveBayes ###\n","  NaiveBayes = GaussianNB()\n","  ## TF-IDF ##\n","  NaiveBayes.fit(train_tfidf,train_class)\n","  pred_NB_TFidf = NaiveBayes.predict(test_tfidf)\n","  predictions_to_csv(test_text, test_class, pred_NB_TFidf, k, train_filename, \"TFidf/GaussianNB\")\n","  ## BERT ##\n","  NaiveBayes.fit(train_encodings,train_class)\n","  pred_NB_Bert = NaiveBayes.predict(test_encodings)\n","  predictions_to_csv(test_text, test_class, pred_NB_Bert, k, train_filename, \"Bert_embeddings/GaussianNB\")\n","\n","  ### Linear SVC ###\n","  LinearSVM = LinearSVC(C=1.0)\n","  ## TF-IDF ##\n","  LinearSVM.fit(train_tfidf, train_class)\n","  pred_SVM_TFidf = LinearSVM.predict(test_tfidf)\n","  predictions_to_csv(test_text, test_class, pred_SVM_TFidf, k, train_filename, \"TFidf/LinearSVM\")\n","  ## BERT ##\n","  LinearSVM.fit(train_encodings, train_class)\n","  pred_SVM_Bert = LinearSVM.predict(test_encodings)\n","  predictions_to_csv(test_text, test_class, pred_SVM_Bert, k, train_filename, \"Bert_embeddings/LinearSVM\")\n","\n","  ### Logistic Regression ###\n","  lr = LogisticRegression(C=11,class_weight = 'balanced')\n","  ## TF-IDF ##\n","  lr.fit(train_tfidf,train_class)\n","  pred_LR_TFidf = lr.predict(test_tfidf)\n","  predictions_to_csv(test_text, test_class, pred_LR_TFidf, k, train_filename, \"TFidf/LogisticRegression\")\n","  ## BERT ##\n","  lr.fit(train_encodings,train_class)\n","  pred_LR_Bert = lr.predict(test_encodings)\n","  predictions_to_csv(test_text, test_class, pred_LR_Bert, k, train_filename, \"Bert_embeddings/LogisticRegression\")\n","\n","  ### Random Forest ###\n","\n","  ## TF-IDF ##\n","  sc_X = StandardScaler(with_mean=False)\n","  X_train_RF_TFidf = sc_X.fit_transform(train_tfidf)\n","  X_test_RF_TFidf = sc_X.transform(test_tfidf)\n","\n","  classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n","  classifier.fit(X_train_RF_TFidf,train_class)\n","  pred_RF_TFidf = classifier.predict(X_test_RF_TFidf)\n","  predictions_to_csv(test_text, test_class, pred_RF_TFidf, k, train_filename, \"TFidf/RandomForest\")\n","\n","  ## BERT ##\n","  X_train_RF_Bert = sc_X.fit_transform(train_encodings)\n","  X_test_RF_Bert = sc_X.transform(test_encodings)\n","\n","  classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n","  classifier.fit(X_train_RF_Bert,train_class)\n","  pred_RF_Bert = classifier.predict(X_test_RF_Bert)\n","  predictions_to_csv(test_text, test_class, pred_RF_Bert, k, train_filename, \"Bert_embeddings/RandomForest\")\n","\n","  ### DecisionTree ###\n","  clf = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)\n","  ## TF-IDF ##\n","  clf = clf.fit(train_tfidf, train_class)\n","  Pred_DT_TFidf = clf.predict(test_tfidf)\n","  predictions_to_csv(test_text, test_class, Pred_DT_TFidf, k, train_filename, \"TFidf/DecisionTree\")\n","  ## BERT ##\n","  clf = clf.fit(train_encodings, train_class)\n","  Pred_DT_Bert = clf.predict(test_encodings)\n","  predictions_to_csv(test_text, test_class, Pred_DT_Bert, k, train_filename, \"Bert_embeddings/DecisionTree\")\n","\n","  ### XGboost ###\n","  classifier = SGDClassifier()\n","  ## TF-IDF ##\n","  classifier.fit(train_tfidf , np.ravel(train_class))\n","  pred_XG_TFidf = classifier.predict(test_tfidf)\n","  predictions_to_csv(test_text, test_class, pred_XG_TFidf, k, train_filename, \"TFidf/XGboost\")\n","  ## BERT ##\n","  classifier.fit(train_encodings , np.ravel(train_class))\n","  pred_XG_Bert = classifier.predict(test_encodings)\n","  predictions_to_csv(test_text, test_class, pred_XG_Bert, k, train_filename, \"Bert_embeddings/XGboost\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### driver function for all baseline tests."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for k in range(0,3): # this will run for every fold we have data for.\n","  data_dict = read_data_for_fold(k)\n","  # print(data_dict.keys())\n","  # for value in data_dict.values():\n","  #   print(value.shape)\n","  test = data_dict.pop(\"test\")\n","  test_text = list(test[\"text\"]) # test data X\n","  test_class = list(test[\"class\"]) # test data Y\n","\n","  Tfidf = TfidfVectorizer(max_features=15000)\n","  sentence_tranformer = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n","\n","  for key in data_dict.keys():\n","    train_text = list(data_dict.get(key)[\"text\"]) # train data X\n","    train_class = list(data_dict.get(key)[\"class\"]) # train data Y\n","    do_baseline_tests(test_text, test_class, train_text, train_class, k ,key, Tfidf, sentence_tranformer)\n","    print(f\"baselines done for {key} in fold {k}\")"]}],"metadata":{"kernelspec":{"display_name":"fyp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9a087ff6df60b7b11c8f40c254358573ffeaae9884c78b2d4011057590ac9004"}}},"nbformat":4,"nbformat_minor":2}
