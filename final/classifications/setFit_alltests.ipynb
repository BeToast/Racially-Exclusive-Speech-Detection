{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/blake/.local/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/blake/.local/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting setfit\n",
      "  Downloading setfit-0.6.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.3.0\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers>=2.2.1\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting evaluate>=0.3.0\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.3.0->setfit) (5.4.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/blake/.local/lib/python3.10/site-packages (from datasets>=2.3.0->setfit) (23.0)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/blake/.local/lib/python3.10/site-packages (from datasets>=2.3.0->setfit) (1.24.2)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: pandas in /home/blake/.local/lib/python3.10/site-packages (from datasets>=2.3.0->setfit) (1.5.3)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets>=2.3.0->setfit) (2.25.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m692.7/887.5 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install setfit\n",
    "%pip install pyarrow\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from os import walk, makedirs\n",
    "from google.colab import files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### output file genertic fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_csv(test_text: list, test_class: list, predictions: list, k: int, train_filename: str, num_epoch: int, pred_output_dir: str):\n",
    "  # output predictions to csv files.\n",
    "  output_df = pd.DataFrame()\n",
    "  output_df[\"text\"] = test_text\n",
    "  output_df[\"class\"] = test_class\n",
    "  # add new predictions as a 3rd column\n",
    "  # dataframe has these columns (test, class, prediction).\n",
    "  output_df[\"predicted\"] = predictions \n",
    "\n",
    "  makedirs(f\"predictions/k_is_{k}/{pred_output_dir}/\", exist_ok=True)\n",
    "  output_df.to_csv(f\"predictions/k_is_{k}/{pred_output_dir}/{num_epoch}epoch_{train_filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setfit_on_pretrained(test_text: list, test_class: list, train_text: list, train_class: list,\n",
    "                        train_filename: str, k: int, pretrainied_model_string: str, num_epoch:int) -> None:\n",
    "\n",
    "  train_dataset = datasets.Dataset(pa.Table.from_arrays([train_text, train_class], [\"text\",\"class\"]))\n",
    "\n",
    "  model = SetFitModel.from_pretrained(pretrainied_model_string)\n",
    "\n",
    "  # total training data for each network is 320 per epoch\n",
    "  # because training data size fluctuates, we need to generate equal number for setfit pairs to have fair training.\n",
    "  # this way we are effectively testing if diversity in data matters.\n",
    "  # for traintest2 : 8*x*2 = 320 , 160/8 = x  <- same math\n",
    "  num_iter = round(160/len(train_text))\n",
    "\n",
    "  trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    batch_size=16,\n",
    "    num_iterations=num_iter, # The number of text pairs to generate for contrastive learning\n",
    "    num_epochs=num_epoch, # The number of epochs to use for contrastive learning\n",
    "    column_mapping={\"text\": \"text\", \"class\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    "  )\n",
    "  # train network\n",
    "  trainer.train()\n",
    "  # predict with network\n",
    "  predictions = trainer.model.predict(test_text)\n",
    "\n",
    "  # output filename\n",
    "  modelnamesplit = pretrainied_model_string.split('/')\n",
    "  modelname = modelnamesplit[len(modelnamesplit)-1]\n",
    "\n",
    "  predictions_to_csv(test_text,\n",
    "                    test_class,\n",
    "                    predictions,\n",
    "                    k,\n",
    "                    train_filename,\n",
    "                    num_epoch,\n",
    "                    modelname\n",
    "                    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### function to read all data in from data_ready/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_for_fold(k: int) -> dict:\n",
    "  data_dict = {}\n",
    "  k_fold_dir = f\"data_ready/k_is_{str(k)}/\"\n",
    "  # print (os.path(data_ready_dir))\n",
    "  filenames = []\n",
    "  for (_, _, name) in walk(k_fold_dir): \n",
    "    filenames.extend(name)\n",
    "\n",
    "  for fname in filenames:\n",
    "    dataframe = pd.read_csv(k_fold_dir+fname).sample(frac=1)\n",
    "    dict_key = fname.split(\".csv\")[0]\n",
    "    data_dict.update({dict_key : dataframe})\n",
    "\n",
    "  return data_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames = ['cardiffnlp/bert-base-multilingual-cased-sentiment-multilingual',\n",
    "              'Hate-speech-CNERG/dehatebert-mono-french',\n",
    "              'morit/french_xlm_xnli',\n",
    "              'cardiffnlp/xlm-roberta-base-sentiment-multilingual']\n",
    "\n",
    "\n",
    "for modelname in modelnames:\n",
    "  for k in range(0,3): # this will run for every fold we have data for.\n",
    "    data_dict = read_data_for_fold(k)\n",
    "    test = data_dict.pop(\"test\")\n",
    "    test_text = list(test[\"text\"]) # test data X\n",
    "    test_class = list(test[\"class\"]) # test data Y\n",
    "    for filename in data_dict.keys():\n",
    "      train_text = list(data_dict.get(filename)[\"text\"]) # train data X\n",
    "      train_class = list(data_dict.get(filename)[\"class\"]) # train data Y\n",
    "      for epochs in range(1,6):\n",
    "        setfit_on_pretrained(\n",
    "          test_text,\n",
    "          test_class,\n",
    "          train_text,\n",
    "          train_class,\n",
    "          filename,\n",
    "          k,\n",
    "          modelname,\n",
    "          epochs\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
